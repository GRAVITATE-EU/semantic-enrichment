#
# eval_attrib_ie config
#

[evaluation]

eval_datasets = [
		#'nyt-clauseIE',
		#'reverb-clauseIE',
		#'wikipedia-clauseIE',
		'attribIE-CH',
		#'attribIE-DBpedia',
	]

[datasets]

# output files (will be appended with _<dataset>.txt)
open_extraction_templates_file=extraction_templates
output_file_pos=pos_labelled_sents
output_file_seeds=seed_tuples
output_file_extraction=extracted_entities

# files to generate during the process
output_pos=True
output_seed=True
output_plain_extracts=True
output_encoded_extracts=True
output_annotated_prop=True

# lexicon files (ranked list entry muct always be first in this list)
list_lexicon_files = [
	#{
	#	'type' : 'ranked_list',
	#	'file' : 'noun_type_CH_ranked_list.txt'
	#},
	#{
	#	'type' : 'plain',
	#	'file' : 'noun_type_CH_lexicon.csv'
	#},
	#{
	#	'type' : 'plain',
	#	'file' : 'pronoun_type_lexicon.csv'
	#},
	#{
	#	'type' : 'skos_json',
	#	'lemma' : 'ch-gazatteer-names.json',
	#	'hyper' : 'ch-gazatteer-hypernym.json',
	#	'related' : 'ch-gazatteer-related.json'
	#},
	#{
	#	'type' : 'nell',
	#	'file' : 'NELL.KBP2013.max3.v1.nps.csv'
	#},
	]

# limit for number of sents before processing stops (-1 for no limit) - note sents processed in text blocks so might get a few extra
max_sent_limit=-1

# random sample of documents used for creating open extraction templates
# note: this needs to be big enough to capture important lexical terms (e.g. 10,000), but small enough the templates can be generated in a reasonable timeframe
random_subset_training=10000

# number of processes to spawn for CPU intensive NLP processes (generate open extraction templates, extraction using templates)
# matching to number of available CPU cores is probably optimal
process_count=4

# choice of POS tagging pattern (propositional | attributional)
# propositional == classic openIE {arg,rel,arg}
# attributional == new openIE (subj,attr), (subj,(attr,obj)) and propositions for an implied subject (attr), ((attr,obj))
# 

#pos_pattern_type=propositional
pos_pattern_type=attributional



# strategy for pruning the seed tuples based on lexicon lookup of arguments
# premissive|selective|strict|no_filter
#strategy_seed_tuples=no_filter
strategy_seed_tuples=strict

# topN open extraction template rules to keep
# note: this needs to be big enough to capture the most important linguistic patterns (e.g. 50,000) BUT not so large it takes 5 minutes to execute the pattern set on each artifact
target_extraction_templates=50000

# relevance feedback using a randomly sampled percentage (per phase) of individual annotations from ground truth
relevance_feedback_phases=0
relevance_feedback_percentage_per_phase=25

[common]

# language code to use
language_codes=['en']

# stanford tagger dir
stanford_tagger_dir=c:\stanford-postagger-full

# stanford dependancy parser dir
stanford_parser_dir=c:\stanford-parser-full

# parser model
model_path=edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz
model_jar=c:\stanford-parser-full\stanford-english-corenlp-2016-10-31-models.jar
model_options=
